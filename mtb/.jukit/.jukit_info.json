{"cmd": "import os\nimport threading\nfrom contextlib import contextmanager\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\n_local = threading.local()\ncounts = 1\n\n\ndef preview(func):\n    def inner(*args, **kwargs):\n        func(*args, **kwargs)\n        plt.show()\n\n    return inner\n\n\ndef save_plt_fig(path=\"./pypic\", hold=\"off\"):\n    \"\"\"保存plt中的图片，按数字顺序为图片命名，默认保存到E:\\\\图片\\\\pypic，可手动指定保存路径（绝对路径！！）\n    另：程序默认画完一张图自动hold off，如需设置hold on需要给hold传入参数\"on\"\n\n    Args:\n        path:\n        hold:\n    \"\"\"\n    global counts\n    if not os.path.exists(path):\n        os.mkdir(path)\n    plt.savefig(f\"{path}/{counts}.jpg\")\n    counts += 1\n    if hold == \"on\":\n        pass\n    else:\n        plt.close()\n\n\n@preview\ndef SavePreview_plt_fig(*args, **kwargs):\n    \"\"\"SavePreview_plt_fig.\n    本函数所有参数都和save_plt_fig相同，详情参见前者\n    在save_plt_fig的基础上利用装饰器实现在保存后展示图片，plt的默认设置好像是执行plt.show()\n    以后自动hold=off，所以这里处理关键字hold没什么意义，在plt那里最后的结果都跟hold=off一样的，\n    但是写都写了，再说万一以后有别的解决办法呢？就不删了\n\n    Args:\n        args:\n        kwargs:\n    \"\"\"\n    track = 1\n    if \"hold\" in kwargs.keys():\n        if kwargs[\"hold\"] == \"on\":\n            track = 0\n        kwargs[\"hold\"] = \"on\"\n    save_plt_fig(*args, **kwargs)\n    if track == 1:\n        plt.close()\n\n\n@contextmanager\ndef acquire(*locks):\n    \"\"\"acquire.\n    这个是抄的网上的锁管理器\n\n    Args:\n        locks:\n    \"\"\"\n    locks = sorted(locks, key=lambda x: id(x))\n\n    acquired = getattr(_local, \"acquired\", [])\n    if acquired and max(id(lock) for lock in acquired) >= id(locks[0]):\n        raise RuntimeError(\"Lock Order Violation\")\n\n    acquired.extend(locks)\n    _local.acquired = acquired\n\n    try:\n        for lock in locks:\n            lock.acquire()\n        yield\n    finally:\n        for lock in reversed(locks):\n            lock.release()\n        lenth = len(locks)\n        del acquired[lenth:]\n\n\ncpu_nums = os.cpu_count()\n\n\ndef grid_caculator_multiprocessing(\n    x,\n    y,\n    calculator,\n    title=\"pic\",\n    xlabel=\"x\",\n    ylabel=\"y\",\n    zlabel=\"z\",\n    draw_pic=True,\n    n_jobs=None,\n):\n    \"\"\"mesh_multiprocessing.本函数与下方的calculate函数共同组成并行计算函数体\n       用法：mp.mesh_multiprocessing(eng, x, y ,' x * np.exp(-x * 2 - y**2)')\n\n    Args:\n        x:\n        y:\n        calculator:\n        title:\n        xlabel:\n        ylabel:\n        zlabel:\n        n_jobs:并行数，默认为cpu核心数量\n    \"\"\"\n    global mpmesh_lsts, cpu_nums, mpmesh_x, mpmesh_y, mpmesh_caculator\n    if n_jobs is None:\n        n_jobs = cpu_nums\n    mpmesh_lsts = np.array_split(x, n_jobs)\n    if isinstance(calculator, str):\n        mpmesh_caculator = eval(f\"lambda x,y:{calculator}\")\n    else:\n        mpmesh_caculator = calculator\n    mpmesh_y = y\n    mpmesh_x = x\n    with multiprocessing.Pool(n_jobs) as p:\n        stack = np.hstack(p.map(calculate, mpmesh_lsts))\n    if draw_pic:\n        fig = go.Figure(data=[go.Surface(x=x, y=y, z=stack)])\n\n        fig.update_layout(\n            title=title, xlabel=xlabel, ylabel=ylabel, zlabel=zlabel  # 标题\n        )\n\n        fig.show()\n    else:\n        return stack\n\n\ndef calculate(x):\n    global mpmesh_y\n    x, y = np.meshgrid(x, mpmesh_y)\n    return mpmesh_caculator(x, y)\n\n\ndef change_col_dtype(DataFrame, before, after):\n    \"\"\"boolcol_to_int.将给定的DateFrame中的所有某(before)类型的列转化为其他(after)类型\n\n    Args:\n        DataFrame:\n        before: 转化前的类型名\n        after: 转化后的类型名\n    \"\"\"\n    for column in DataFrame.columns:\n        if DataFrame[column].dtype == before:\n            DataFrame[column] = DataFrame[column].astype(after)\n    return DataFrame\n\n\ndef corr_heatmap(DataFrame, title=\"pic\"):\n    \"\"\"heatmap.快速绘制出一个含有数字的DataFrame的相关系数热力图\n\n    Args:\n        DataFrame: pd.DataFrame\n        title:\n    \"\"\"\n    from seaborn import heatmap\n\n    DataFrame = change_col_dtype(DataFrame, bool, int)\n    numeric_columns = DataFrame.select_dtypes(include=[\"number\"])\n    heatmap(numeric_columns.corr(), annot=True)\n    plt.title(title)\n    plt.show()\n\n\ndef fast_corrscatter_evaluate(DataFrame, target, n=4):\n    \"\"\"fast_corrscatter_evaluate.快速绘制相关系数散点图（挑和target关联最大的四个）\n\n    Args:\n        DataFrame:\n        target:\n    \"\"\"\n    from pandas.plotting import scatter_matrix\n\n    DataFrame = change_col_dtype(DataFrame, bool, int)\n    numeric_columns = DataFrame.select_dtypes(include=[\"number\"])\n    attribute = numeric_columns.corr()[target].nlargest(n).index.tolist()\n    scatter_matrix(DataFrame[attribute])\n\n\ndef sklearn_model_report(model, train_data, label, scoring=\"accuracy\"):\n    \"\"\"sklearn_model_report.本函数用于输出已训练好的sklearn模型的各项性能参数\n\n    Args:\n        model: 已训练好的模型\n        train_data: 数据集（不含结果）\n        label: train_data对应的正确结果\n        scoring: 最后要输出的参数的名称，如accuracy, precision, recall，如果只想求一个就输入字符串，否则用一个列表框起来\n    \"\"\"\n    from sklearn.metrics import confusion_matrix, classification_report\n    from sklearn.model_selection import cross_val_score\n\n    pred = model.predict(train_data)\n    print(\"混淆矩阵如下：\")\n    print(confusion_matrix(label, pred), \"\\n\")\n    print(\"查全率查准率等各项指标如下：\")\n    print(classification_report(label, pred))\n\n    if isinstance(scoring, list):\n        for i in scoring:\n            _train = cross_val_score(model, train_data, label, scoring=i)\n            print(f\"在给出的训练集上，本模型{i}指标的多次预测平均值为：{_train.mean()}\")\n\n    if isinstance(scoring, str):\n        _train = cross_val_score(model, train_data, label, scoring=scoring)\n        print(f\"在给出的训练集上，本模型{scoring}指标的多次预测平均值为：{_train.mean()}\")\n\n\ndef general_clf_report(predicted_data, label):\n    from sklearn.metrics import confusion_matrix, classification_report\n\n    if not isinstance(predicted_data, list):\n        print(\"混淆矩阵如下\")\n        print(confusion_matrix(label, predicted_data), \"\\n\")\n        print(\"查全率查准率等各项指标如下：\")\n        print(classification_report(label, predicted_data))\n    else:\n        ...\n\n\ndef confusion_matrix_analysis(confusion_matrix, title1=\"混淆矩阵热力图\", title2=\"错误率热力图\"):\n    \"\"\"confusion_matrix_analysis. 输出俩图，第一个图是混淆矩阵的热力图\n    第二个图里面每一行是代表准确值，每一列代表预测值，所以每一个格子里的值代表某一准确值被预测为某错误的预测值的概率\n    返回precision, recall, false_positive_rate\n\n    Args:\n        confusion_matrix:\n    Returns:\n        precision: 查准率/精度\n        recall: 查全率/召回率/真正例率\n        false_positive_rate: 假正例率\n        f1_score: f1分数\n        accuracy: 准确率\n        macro_precision: 宏查准率\n        macro_recall: 宏查全率\n        macro_f1: 宏F1\n        micro_precision: 微查准率\n        micro_recall: 微查全率\n        micro_f1: 微F1\n        weighed_precision: 带权查准率\n        weighed_recall: 带权查全率\n        weighed_f1: 带权F1\n    \"\"\"\n    from seaborn import heatmap\n\n    fig, ax = plt.subplots(1, 2)\n    subplot1 = heatmap(confusion_matrix, annot=True, ax=ax[0])\n    subplot1.set_title(title1)\n\n    row_sum = confusion_matrix.sum(axis=1, keepdims=True)\n    col_sum = confusion_matrix.sum(axis=0, keepdims=True).ravel()\n    all_sum = confusion_matrix.sum()\n\n    norm_confusion_matrix = confusion_matrix / row_sum\n    # 上面一行需要把每行的每一列除对应行的和，所以不能在最开始就降维（不然不同列就除的和就不一样了）\n    row_sum = row_sum.ravel()\n\n    np.fill_diagonal(norm_confusion_matrix, 0)\n    subplot2 = heatmap(norm_confusion_matrix, annot=True, ax=ax[1])\n    subplot2.set_title(title2)\n    plt.show()\n\n    true_positive = np.diagonal(confusion_matrix)\n    false_positive = col_sum - true_positive\n    true_negative = [\n        true_positive.sum() - true_positive[i] for i in range(len(true_positive))\n    ]\n\n    recall = true_positive / row_sum\n    precision = true_positive / col_sum\n    false_positive_rate = false_positive / (false_positive + true_negative)\n    accuracy = true_positive.sum() / all_sum\n\n    f1_score = 2 / ((1 / precision) + (1 / recall))\n\n    macro_precision = sum(precision) / len(precision)\n    macro_recall = sum(recall) / len(recall)\n    macro_f1 = (2 * macro_precision * macro_recall) / (macro_precision + macro_recall)\n\n    micro_precision = sum(true_positive) / (sum(true_positive) + sum(false_positive))\n    micro_recall = sum(true_positive) / sum(row_sum)\n    micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n\n    weigh = np.array([row_sum / all_sum]).ravel()\n    weighed_precision = sum(weigh * precision)\n    weighed_recall = sum(weigh * recall)\n    weighed_f1 = (2 * weighed_precision * weighed_recall) / (\n        weighed_precision + weighed_recall\n    )\n    print(\"{:-^45}\".format(\"以下为各类的数量\"))\n    print(f\"实际数量：{row_sum}\")\n    print(f\"预测数量：{col_sum}\")\n\n    print(\"{:-^45}\".format(\"以下为以各类分别作为正例时的各项指标\"))\n    print(f\"查准率(精度)为\\n{precision}\\n\")\n    print(f\"查全率(真正例率/召回率)为\\n{recall}\\n\")\n    print(f\"假正例率为\\n{false_positive_rate}\\n\")\n    print(f\"F1分数为\\n{f1_score}\\n\")\n    print(f\"准确率为\\n{accuracy}\\n\")\n\n    print(\"{:-^45}\".format(\"以下为综合指标\"))\n    print(f\"宏查准率为                      {macro_precision}\")\n    print(f\"宏查全率为                      {macro_recall}\")\n    print(f\"宏F1为                          {macro_f1}\\n\")\n    print(f\"微查准率为                      {micro_precision}\")\n    print(f\"微查全率为                      {micro_recall}\")\n    print(f\"微F1为                          {micro_f1}\\n\")\n    print(f\"加权查准率为                    {weighed_precision}\")\n    print(f\"加权查全率为                    {weighed_recall}\")\n    print(f\"加权F1为                        {weighed_f1}\")\n\n    return (\n        precision,\n        recall,\n        false_positive_rate,\n        f1_score,\n        accuracy,\n        macro_precision,\n        macro_recall,\n        macro_f1,\n        micro_precision,\n        micro_recall,\n        micro_f1,\n        weighed_precision,\n        weighed_recall,\n        weighed_f1,\n    )\n\n\ndef learning_curve(model, x, y, title=\"学习曲线\"):\n    \"\"\"learning_curve. 简单的学习曲线函数，sklearn的函数具体还不知道是个什么情况\n\n    Args:\n        model: 未训练好的模型\n        x:\n        y:\n        title:\n    Returns:\n        model: 训练好的模型\n    \"\"\"\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n\n    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2)\n    train_errors, test_errors = [], []\n    for m in range(1, len(train_x)):\n        model.fit(train_x[:m], train_y[:m])\n        train_y_predict = model.predict(train_x[:m])\n        test_y_predict = model.predict(test_x)\n        train_errors.append(mean_squared_error(train_y[:m], train_y_predict))\n        test_errors.append(mean_squared_error(test_y, test_y_predict))\n    RMSE_train = np.sqrt(train_errors)\n    RMSE_test = np.sqrt(test_errors)\n    plt.plot(RMSE_train, label=\"训练集\")\n    plt.plot(RMSE_test, \"-+\", label=\"测试集\")\n    plt.xlabel(\"训练集大小\")\n    plt.ylabel(\"误差(RMSE)\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n    if len(test_errors) >= 10:\n        delta = np.abs(RMSE_train[-10:] - RMSE_test[-10:])\n        delta = 2 * delta / (RMSE_train[-10:] + RMSE_test[-10:])\n        delta = delta.mean()\n        if delta <= 0.1:\n            print(f\"训练集和测试集的误差相差{delta*100}%, 结果不一定好，注意一下误差，可能会过拟合\")\n        else:\n            print(f\"训练集和测试集的误差相差{delta*100}%, 可能是欠拟合\")\n    return model\n\n\nclass Grey_model_11:\n    def __init__(self):\n        self.c = 0\n        self.level = np.array([])\n\n    def fit(self, x):\n        \"\"\"fit. 这个函数进行级比检验，为了跟sklearn习惯接轨，所以叫fit了。\n        这个平移的c求得比较玄学，我不知道为什么这么取，但是管用。\n        c是每次递归时（递归的同时x自加c）累加x的最小值。\n\n        Args:\n            x: 仅接收列表和一维np数组，目前不想导入pd库\n        \"\"\"\n        if isinstance(x, list):\n            self.x = np.array(x)\n        elif isinstance(x, np.ndarray):\n            self.x = x\n        elif isinstance(x, pd.Series):\n            self.x = x.values\n        self.level = self.x[:-1] / self.x[1:]\n        lenth = len(self.x)\n        if self.level.min() >= np.exp(-2 / (lenth + 1)) and self.level.max() <= np.exp(\n            2 / (lenth + 2)\n        ):\n            print(f\"级比检验完成，符合要求，平移了{self.c}，即c = {self.c}\")\n        else:\n            self.c += self.x.min()\n            self.x += self.x.min()\n            self.fit(self.x)\n\n    def predict(self, n):\n        X = []\n        tmp = 0\n        for i in self.x:\n            tmp += i\n            X.append(tmp)\n        X = np.array(X)\n        Z = -0.5 * (X[1:] + X[:-1])\n        Y = self.x[1:].T\n        B = np.c_[Z.T, np.ones(len(Z)).T]\n        result = np.linalg.inv((B.T.dot(B))).dot(B.T).dot(Y)\n        a = result[0]\n        b = result[1]\n        print(f\"发展系数a = {a}\\n灰作用量b = {b}\")\n\n        predict_X = []\n\n        for i in range(len(X) + n):\n            predict_X.append((self.x[0] - b / a) * np.exp(-a * i) + b / a)\n        predict_X = np.array(predict_X)\n        verfify = predict_X[: len(X)].copy()\n        predict = predict_X[len(X) - 1 :].copy()\n        verfify_x0 = verfify[1:] - verfify[:-1] - self.c\n        predict_x0 = predict[1:] - predict[:-1] - self.c\n        print(f\"预测结果为{predict_x0}\")\n\n        delta = np.abs(self.x[1:] - self.c - verfify_x0) / (self.x[1:] - self.c)\n        delta = np.r_[0, delta]\n        print(f\"相对误差为{delta}\")\n        if np.array(delta < 0.1).all():\n            print(\"相对误差检验达到了较高的要求\")\n        elif np.array(delta < 0.2).all():\n            print(\"相对误差检验达到了一般的要求\")\n        else:\n            print(\"相对误差检验不合格\")\n\n        rho = 1 - ((1 - 0.5 * a) / (1 + 0.5 * a)) * self.level\n        print(f\"级比偏差为{rho}\")\n        if np.array(np.abs(rho) < 0.1).all():\n            print(\"级比偏差检验达到了较高的要求\")\n        elif np.array(np.abs(rho) < 0.2).all():\n            print(\"级比偏差检验达到了一般的要求\")\n        else:\n            print(\"级比偏差检验不合格\")\n\n        print_verfify_x0 = np.r_[self.x[0] - self.c, verfify_x0]\n\n        report = pd.DataFrame()\n        report[\"序号\"] = np.arange(1, len(self.x) + 1)\n        report[\"原始值\"] = self.x\n        report[\"预测值\"] = print_verfify_x0\n        report[\"残差\"] = self.x - self.c - print_verfify_x0\n        report[\"相对误差\"] = delta\n        report[\"级比偏差\"] = np.r_[0, rho]\n        print(report)\n\n\nfrom tools import prefer_settings\nprefer_settings()\ngm_pred = Grey_model_11()\ngm_pred.fit([71.1, 72.4, 72.4, 72.1, 71.4, 72, 71.6])\ngm_pred.predict(5)\n", "cmd_opts": " --cell_id=NONE -s", "import_complete": 1, "terminal": "nvimterm"}